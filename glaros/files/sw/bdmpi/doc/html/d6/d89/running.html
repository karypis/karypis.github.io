<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.5"/>
<title>BDMPI - Big Data Message Passing Interface: Running BDMPI Programs</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../resize.js"></script>
<script type="text/javascript" src="../../navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">BDMPI - Big Data Message Passing Interface
   &#160;<span id="projectnumber">Release 0.1</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.5 -->
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('d6/d89/running.html','../../');});
</script>
<div id="doc-content">
<div class="header">
  <div class="headertitle">
<div class="title">Running BDMPI Programs </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#onenode">Running on a single node</a></li>
<li class="level1"><a href="#manynodes">Running on multiple nodes</a></li>
<li class="level1"><a href="#bdmprun">Options of bdmprun</a><ul><li class="level2"><a href="#execoptions">Options related to the execution environment</a></li>
<li class="level2"><a href="#memoptions">Options related to memory resources</a></li>
<li class="level2"><a href="#otheroptions">The remaining options</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p>For the purpose of the discussion in this section, we will use the simple "Hello
world" BDMPI program that is located in the file <code>test/helloworld.c</code> of BDMPI's source distribution. This program is listed bellow:</p>
<dl class="section user"><dt></dt><dd><pre class="fragment">#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;sys/types.h&gt;
#include &lt;unistd.h&gt;
#include &lt;bdmpi.h&gt;

int main(int argc, char *argv[])
{
  int i, myrank, npes;

  MPI_Init(&amp;argc, &amp;argv);

  MPI_Comm_size(MPI_COMM_WORLD, &amp;npes);
  MPI_Comm_rank(MPI_COMM_WORLD, &amp;myrank);

  for (i=0; i&lt;npes; i++) {
    MPI_Barrier(MPI_COMM_WORLD);
    if (myrank == i) {
      printf("[%5d:%5d] Hello from rank: %2d out of %2d\n", 
          (int)getppid(), (int)getpid(), myrank, npes);
      fflush(stdout);
    }
  }
  MPI_Barrier(MPI_COMM_WORLD);

  MPI_Finalize();

  /* It is important to exit with a EXIT_SUCCESS status */
  return EXIT_SUCCESS; 
}

</pre></dd></dl>
<p>This program has its process print its parent and own process ID along with its rank in the <code>MPI_WORLD_COMM</code> and the size of that communicator. Note that the calls to MPI_Barrier() are used to ensure that the output from the different processes is displayed in an orderly fashion.</p>
<p>This program can be compiled by simply executing </p>
<pre class="fragment">bdmpicc -o helloworld test/helloworld.c
</pre><p> and is also been automatically built during BDMPI's build process and resides in the <code>build/Linux-x86_64/test</code> directory.</p>
<hr/>
 <h1><a class="anchor" id="onenode"></a>
Running on a single node</h1>
<p>BDMPI provides the <code>bdmprun</code> command to run BDMPI program on a single node. For example, the following command</p>
<pre class="fragment">bdmprun -ns 4 build/Linux-x86_64/test/helloworld
</pre><p>will create a single master process that will spawn four slave processes (i.e., due to <code>-ns 4</code> option), each executing the <code>helloworld</code> program. Here is a sample output of the above execution:</p>
<dl class="section user"><dt></dt><dd><pre class="fragment">[28933:28936] Hello from rank:  0 out of  4
[28933:28937] Hello from rank:  1 out of  4
[28933:28938] Hello from rank:  2 out of  4
[28933:28939] Hello from rank:  3 out of  4
[ bd4-umh: 28933]------------------------------------------------
[ bd4-umh: 28933]Master 0 is done.
[ bd4-umh: 28933]------------------------------------------------
[ bd4-umh: 28933]Master timings
[ bd4-umh: 28933]    totalTmr:      0.022s
[ bd4-umh: 28933]    routeTmr:      0.001s
[ bd4-umh: 28933]------------------------------------------------
</pre></dd></dl>
<p>The first four lines came from the <code>helloworld</code> program itself, whereas the remaining output lines came from <code>bdmprun</code>. Looking at this output we can see that all four processes share the same parent process, which corresponds to the master process, <code>bdmprun</code>. The master process reports the timing statistics at the end of the execution of <code>helloworld</code>.</p>
<hr/>
 <h1><a class="anchor" id="manynodes"></a>
Running on multiple nodes</h1>
<p>In order for a program to run at multiple nodes, <code>bdmprun</code> needs to be combined with the <code>mpiexec</code> command that is provided by MPI. For example, the following command</p>
<pre class="fragment">mpiexec -hostfile ~/machines -np 3 bdmprun -ns 4 build/Linux-x86_64/test/helloworld
</pre><p>will start three master processes (i.e., due to <code>-np 3</code>) and each master process will spawn four slaves, resulting at a total of 12 processes executing the <code>helloworld</code> program. Here is a sample output of the above execution:</p>
<dl class="section user"><dt></dt><dd><pre class="fragment">[  331:  334] Hello from rank:  0 out of 12
[  331:  335] Hello from rank:  1 out of 12
[  331:  336] Hello from rank:  2 out of 12
[  331:  337] Hello from rank:  3 out of 12
[27162:27165] Hello from rank:  4 out of 12
[27162:27166] Hello from rank:  5 out of 12
[27162:27167] Hello from rank:  6 out of 12
[27162:27168] Hello from rank:  7 out of 12
[23530:23533] Hello from rank:  8 out of 12
[23530:23534] Hello from rank:  9 out of 12
[23530:23535] Hello from rank: 10 out of 12
[23530:23536] Hello from rank: 11 out of 12
[ bd1-umh:   331]------------------------------------------------
[ bd1-umh:   331]Master 0 is done.
[ bd2-umh: 27162]------------------------------------------------
[ bd2-umh: 27162]Master 1 is done.
[ bd3-umh: 23530]------------------------------------------------
[ bd3-umh: 23530]Master 2 is done.
[ bd1-umh:   331]------------------------------------------------
[ bd1-umh:   331]Master timings
[ bd1-umh:   331]    totalTmr:      0.058s
[ bd1-umh:   331]    routeTmr:      0.001s
[ bd1-umh:   331]------------------------------------------------
</pre></dd></dl>
<p>Notice that the parent process IDs of the different BDMPI processes reveal the master-slave relation that exists (e.g., ranks 0&ndash;3, 4&ndash;7, and 8&ndash;11 have the same parent process). Also, the output lines generated by <code>bdmprun</code> provide information as to the machine on which the particular master process was executed (e.g., <code>bd1-umh</code>, <code>bd2-umh</code>, and <code>bd3-umh</code>) and overall timing information. The names of these machines are specified in the  machines file that was provided as the <code>-hostfile</code> of <code>mpiexec</code>, following <code>mpiexec's</code> host machine specification guidelines (see MPICH documentation).</p>
<p>Do not configure your hostfile to have <code>mpiexec</code> start multiple processes on the same node. If you do, then each node will have multiple master processes running on it (i.e., multiple instances of <code>bdmprun</code>), each of which will have its own set of slave processes. If you wish to have multiple slave processes run at once on the same node, use the <code>-nr</code> option instead.</p>
<hr/>
 <h1><a class="anchor" id="bdmprun"></a>
Options of bdmprun</h1>
<p>There are a number of optional parameters that control <code>bdmprun</code>'s execution. You can get a list of these options by simply executing <code>bdmprun -h</code>, which will display the following:</p>
<dl class="section user"><dt></dt><dd><pre class="fragment"> 
Usage: bdmprun [options] exefile [options for the exe-file]
 
 Required parameters
    exefile     The program to be executed.
 
 Optional parameters
  -ns=int [Default: 1]
     Specifies the number of slave processes on each node.
 
  -nr=int [Default: 1]
     Specifies the maximum number of concurrently running slaves.
 
  -nc=int [Default: 1]
     Specifies the maximum number of slaves in a critical section.
 
  -sm=int [Default: 20]
     Specifies the number of shared memory pages allocated for each slave.
 
  -im=int [Default: 4]
     Specifies the maximum size of a message that will be buffered
     in the memory of the master. Messages longer than that are buffered
     on disk. The size is in terms of memory pages.
 
  -mm=int [Default: 32]
     Specifies the maximum size of the buffer to be used by MPI for 
     inter-node communication. The size is in terms of memory pages.
 
  -sb=int [Default: 32]
     Specifies the size of allocations for which the explicit storage backed
     subsystem should be used. The size is in terms of number of pages and a
     value of 0 turns it off.
 
  -wd=string [Default: /tmp/bdmpi]
     Specifies where working files will be stored.
 
  -dl=int [Default: 0]
     Selects the dbglvl.
 
  -h
     Prints this message.
</pre></dd></dl>
<h2><a class="anchor" id="execoptions"></a>
Options related to the execution environment</h2>
<p>The <code>-ns</code>, <code>-nr</code>, <code>-nc</code>, and <code>-wd</code> options are used to control the execution environment of a BDMPI job.</p>
<p>The <code>-ns</code> option specifies the number of slaves that <code>bdmprun</code> will spawn on each node. These slaves then proceed to execute the provided BDMPI program (i.e., <code>exefile</code>). If multiple instances of <code>bdmprun</code> are started via <code>mpiexec</code>, then the total number of processes that are involved in the parallel execution of <code>exefile</code> is <code>np*ns</code>, where <code>np</code> is the number of processes specified in <code>mpiexec</code> and <code>ns</code> is the number of slaves supplied via the <code>-ns</code> option. In other words, the size of the <code>MPI_COMM_WORLD</code> communicator is <code>np*ns</code>.</p>
<p>The <code>-nr</code> options specifies the number of slave processes spawned by a single <code>bdmprun</code> process that are allowed to be executing concurrently. This option enables BDMPI's node-level cooperative multitasking execution model, which is designed to ensure that the aggregate memory requirements of the concurrently executing processes do not overwhelm the amount of available physical memory on the system. The default value for <code>-nr</code> is one, allowing only one slave to be executing per master process at a time. However, for multi-core systems, <code>-nr</code> can be increased as long as the aggregate amount of memory required by the concurrently running processes can comfortably fit within the available memory of the system. For example,</p>
<pre class="fragment">mpiexec -hostfile ~/machines -np 3 bdmprun -ns 4 -nr 2 build/Linux-x86_64/test/helloworld
</pre><p>will allow a maximum of two slave processes on each of the three nodes to be executing concurrently. The value specified for <code>-nr</code> should be less than or equal to <code>-ns</code>.</p>
<p>Besides the aggregate memory requirements, another issue that needs to be considered when the number of running processes is increased is the capability of the underlying I/O subsystem to handle concurrent I/O operations. On some systems, a faster execution can be obtained by allowing multiple slave processes to run concurrently but reduce the concurrency allowed during I/O operations. To facilitate this, BDMPI provides a pair of API functions that implement critical sections, which limit the number of processes that can be at a critical section at any give time. These are described in <a class="el" href="../../d8/daa/api.html#bdmpimutex">Intra-slave synchronization</a>. The number of slave processes that are allowed to be running at a critical section is controlled by <code>bdmprun</code>'s <code>-nc</code> option, which should be less than or equal to the value specified for <code>-nr</code>.</p>
<p>Finally, the <code>-wd</code> option specifies the name of the directory that BDMPI will use to store the various intermediate file that it generates. Since BDMPI's execution is primarily out-of-core, this directory should be on a drive/volume that has a sufficient amount of available storage and preferably the underlying hardware should support fast I/O.</p>
<h2><a class="anchor" id="memoptions"></a>
Options related to memory resources</h2>
<p>The <code>-sm</code>, <code>-im</code>, <code>-mm</code>, and <code>-sb</code> options are used to control various aspects of the execution of a BDMPI program as it relates to how it uses the memory. Among them, <code>-im</code> and <code>-sb</code> are the most important, whereas <code>-sm</code> and <code>-mm</code> are provided for fine tuning and most users will not need to modify them. Note that for all memory-related options, the size is specified in terms of memory pages and not bytes. On Linux, a memory page is typically 4096 bytes.</p>
<p>Since processes in BDMPI can be suspended and as such are not available to receive data sent to them, the master processes buffer data at the message's destination node. The <code>-im</code> parameter controls if the message will be buffered in DRAM or if it will buffered on disk. Messages whose size is smaller than the value specified by <code>-im</code> are buffered in memory, otherwise they are buffered on disk. The only exception to the above rule are the broadcast and the reduction operations, in which the buffering is always done in memory. Also note that in the case of collective communication operations, the size of the message is defined to be the amount of data that any processor will need to send/receive to/from any other processor.</p>
<p>As discussed in <a class="el" href="../../d1/dfb/intro.html#sbmalloc">Efficient loading &amp; saving of a process's address space</a>, BDMPI uses its own dynamic memory allocation library that bypasses the system's swap file and preserves the data and virtual memory mappings throughout the execution of the program. The <code>-sb</code> parameter controls the size of the allocations that will be handled by the sbmalloc subsystem. Any allocation that is smaller than the value specified by <code>-sb</code>, is handled by the standard <code>malloc()</code> library, whereas the rest of the allocations are handled by sbmalloc. If you want to entirely disable sbmalloc, you can specify 0 as the value for this option.</p>
<p>The <code>-sm</code> option specifies the amount of shared memory to allocate for each slave process. This memory is used to facilitate fast communication between the master and the slave processes via <code>memcpy()</code>. The <code>-mm</code> option specifies the size of the buffer that a master process will allocate for inter-node communication. Note that if the data that needs to be communicated between the master and the slaves or other masters exceeds is greater than what is specified by these options, the transfer is done in multiple steps. Thus, the values of these options do not impact the correctness of the execution, but if they are relatively small, may lead to (somewhat) higher communication cost.</p>
<h2><a class="anchor" id="otheroptions"></a>
The remaining options</h2>
<p>The last two options, <code>-dl</code> and <code>-h</code>, are used to turn on various reporting messages and display <code>bdmprun</code>'s help page. Setting <code>-dl</code> to anything else than 0 can potentially generate a lot of reporting messages and is not encouraged. It is there primarily for debugging BDMPI during its development. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated on Mon Apr 21 2014 14:23:11 for BDMPI - Big Data Message Passing Interface by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="../../doxygen.png" alt="doxygen"/></a> 1.8.5 </li>
  </ul>
</div>
</body>
</html>
